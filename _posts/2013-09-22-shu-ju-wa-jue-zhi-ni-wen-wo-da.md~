---
layout: post
title: "数据挖掘之你问我答"
description: "行业运作及挖掘技巧"
category: td
tags: ["数据挖掘","思路"]
---
{% include JB/setup %}

本文转载自[数据科学与R](http://xccds1977.blogspot.com/)翻译Tim Graettinger[文章](http://www.discoverycorpsinc.com/grab-bag-frequently-asked-data/)

## 问题一：你推荐什么样的数据挖掘工具？

n.var <- 20
n.obs <- 200
x <- data.frame(V = matrix(rnorm(n.var*n.obs), n.obs, n.var))
n.dep <- floor(n.var/5)
cat( "Number of dependent variables is", n.dep, "\n")
m <- diag(n.dep:1)


### 首先

我们需要搞清楚一些东西：你的数据挖掘目标是什么？你的团队怎么样？目前正在使用的工具和能力如何？

先想想你的数据挖掘目标。你是打算以一次性的项目来解决单一业务问题？或者你要在企业内部建立数据挖掘的核心能力？你的目标将影响你选择的软件工具。此外，你的预算是否支持你的既定目标？这里面还要考虑到培训和支持成本。

下面请允许我提出两个更广泛的建议：
* 写一个“愿望清单”。假设资金不成问题的话，你会买什么样的工具？为什么？真的，写下你的选择。这个愿望清单在后来是非常有用的。

* 慢慢起步。在开始做数据挖掘项目时，弄清楚你到底需要什么样的工具。根据实际需要去购买软件。

### 其次

密切关注你团队的技能和背景。他们是MBA或者业务专家吗？如果是这样，你可能要考虑使用行业或特定应用的软件工具（例如，贷款审批或欺诈检测）以发挥自己的长处。相反，你的团队主要是由统计人员和分析师组成吗？那么装备更丰富的技术工具软件会使他们表现更好。这些软件通常允许更灵活的数据转换和建模方式（回归，决策树，神经网络等）。在任何情况下，关键是你所使用的工具应该是适合团队技能的，适合他们自身的工作和思考方式，并适合业务和数据挖掘应用以及预测分析。

### 第三

考虑你已经拥有和购买的软件工具。将它与“愿望清单”进行比较。寻找二者的相似和差异部分。根据我的经验，数据挖掘团队需要使用不同的工具来完成其工作。当然，我本人也使用各种软件工具以共同完成以下主要任务：

* ETL（数据的提取，转换和读入）：将原始数据进行旋转，汇集，或是再制到一个单一的分析性的文件中。

* EDA（探索性数据分析）：计算各种统计量，创建交叉联列表，进行数据可视化，并最终解释和理解数据。

* Modeling（建模）：建立回归，决策树，神经网络等模型。在这里一个重要的考虑因素是要编写代码使模型能运行在另一个环境中。

* Reporting and Presentation（报告和呈现）：展现数据挖掘工作的结果，监测随着时间的推移现实世界模型结果的变化。其中有用的工具可能包括很普通的如Excel或PowerPoint，也可能是非常专业，这需要根据你的的业务运营和流程来调整。

### 最后

让我们谈谈你的工具搜索策略吧。现在你已经清楚自己和团队的目标，你们现有的能力和需求。我认为[KDNuggets.com](http://www.kdnuggets.com/)是了解数据挖掘信息的起点。该网站包括了丰富的行业和应用软件产品，介绍了大量的商业或开源软件。

我也希望你加入专业的社交网站，如LinkedIn和AnalyticBridge的小组讨论。你会从过去的贴子中获得丰富而不凡的见解。你可以发表你自己的具体问题，与相似经历或行业技术的人一同讨论。祝你好运！

## 问题二：如何使决策者购买我的数据挖掘项目？

一般我都是以自身经验有感而发的，所以，我谈论的这些东西并非基于对人或企业行为的详尽研究。这些只是我的所思所想，而且对我来说非常合适。

### 一、寻找一个“痛处”

“痛处”是一种业务需要或业务问题，而你有信心通过数据挖掘来解决这个问题。你所专注的这个问题的范围要小，也就是说相对来讲自成体系，有着适当的风险回报比率。这个问题还应该是你和你的的团队非常熟悉的，即对其数据来源和业务流程均很了解。这样团队就可会努力获取一个又一个小范围的成功。因为相对于失败的大项目，随着时间的推移，人们更容易接受一连串小范围成功。

请允许我仔细解释一下失败的大项目和禁忌。在我的职业生涯中，我经常被空降到一些“麻烦”的项目上。这些项目通常会有以下这些特点：


* 一个初出茅庐的新团队好高骛远，试图解决一个非常复杂的、多方面的、高风险的问题。

* 团队完全没有专注于业务问题，而被数据挖掘的炫目技术所迷惑。

* 团队需要在企业积累的海量数据中去大海捞针。

### 二、精心计划

精心规划你的方案，这个方案重点在于关注业务问题，并且解释你的数据挖掘方法如何来解决这个问题。

### 三、热情

你是在推销自己和团队，而不只是一个问题的解决方案。你需要说服资助项目的决策人，你可以使其项目获得成功。如果你去问任何一个风投，“在20分钟的推销时间内你关注什么东西？”他们会告诉你，他们是在看一个人怎么样，和一些特别的想法或解决方案相比，人的因素更为重要。

### 四、坚持

或早或晚总有人会说“不”。了解反对意见是什么，并找到​​解决这些问题的途径。开拓你的的人际网络，寻找其他可以鼓励、引导、并给你真实反馈的人。

我希望你从上述内容中学到的推销技术并不只适用于数据挖掘。对于我来说，对一个组织来说，持续的观察和学习其他成功范例是至关重要的。请他们来指导你，请他们来评价你的方法和你的方案。你将从一个数据挖掘项目中获得更多东西。

## 问题三：进行数据挖掘需要多少数据？

这是迄今为止关于数据挖掘最常见的问题，这个问题得到如此多的关注是有其原因的。当你第一次进行数据挖掘，几乎会下意识的提出这个问题。你想知道，从数据挖掘的角度来讲手里的数据是否足够。尽管问题很简单，但在没有更深理解问题的前提下回答它却是不明智的。在这里我只能提供一些原则性的指导意见，然后你可就此来做为进一步工作的起点。我甚至会提供一个经验法则来帮助你估计所需要的数据。

### 第一个原则是基于关系的复杂度。

你想构建的关系越复杂，你就需要越多的数据来准确的构建它。但是，你可能会说，“我不知道关系有多复杂啊”。从现实角度来看，关系的复杂度可以从影响关系的因子来加以判断。比方说，要预测天气。那么想想可能的影响因素会有哪些，例如风力、风向、温度、湿度、云层等等。你需要对可能的影响因素有大体的估计。


下一步你要考虑，在这些因素条件下，我应该如何从实验中收集数据。至少，你要独立测试每个因子的大小，以判断这些因子间是否相互影响。而且，你要运行每个实验多次，以减少噪音的影响。我们可以将这些因素通盘考虑后转化为一个经验公式：

NR ≥ M × 2(F+1)

其中F是影响因子个数，M是每个实验的倍数（我们可以取25），2代表每个因子有两种水平需要测量。NR就是最终的结果，也就是你需要的最少数据个数。让我们来试一个简单的例子。假设确定有9个因子，乘数为25，根据经验法则，我们得到：

 NR ≥ (25) × 2(9+1) ≈ 25,000

你会注意到，随着数据挖掘中的因子增加，所需要的数据以指数倍增加。有人曾认真地告诉我，他的模型中往往有50甚至100个因子，所需数据真是个天文数字。而我的回答是，并非所有因子都会独立施加影响（意味着有些因子是无需考虑的）。如果你的模型中有很多因子，那就用F = 12作为一个良好的开端。

### 第二个原则是平衡，尤其是各输出结果的平衡。

例如在天气预测的例子里，模型产生两种输出结果，即天睛或雨天，而每种结果都需要有相应的数据。因此你的模型输出结果越多，你就需要更多的数据。

不仅如此，每项输出结果还要有足够的“混合”。还是以天气预测的例子来讲，假设你有10万个样本记录，但其中只有1％的数据是对应着雨天的结果。换句话说，雨天这个输出结果只对应1000个样本。对于10万来讲，1000条记录确实不多，而这个最低频数样本将成为建模的约束条件。在前文所述的经验公式中，NR实际上是指的最低频数结果所对应的样本数，思考一下原因。

### 第三个原则是模型的复杂性

。模型越复杂，那么涉及到的参数或系数会越多，则需要更多的数据。但关于这一原则还有更多需要讨论的细节，我们下回再说。

## 问题四：我的模型在训练数据上表现很好，为什么在新数据上表现很糟？

你遇到的就是被称为“过度拟合”(over-fit)的典型症状。通常是在为稀有事件建模时较为容易产生这种问题，例如发动机故障预测。

当某种输出结果非常罕见（如百分之一或千分之一）时，就会倾向于建立很复杂的模型。复杂的模型会有很多参数，用来预测或者说“适应”训练数据。问题是，该模型包含了训练数据所独有的特征，但在其它地方并不一定适用。这就像为你自己量身定制的衣服只适合于你自己，但并不适合其他人。在建模的世界里出现过度拟合是糟糕的。我们要构建的是稳健的、能概括新数据的模型。

### **你能做些什么呢？**

**首先**要将样本进行随机划分，分别建立训练、测试和验证这三个数据集。这样，你可以检测出模型的过度拟合问题。

**其次**，计算模型的拟合比率。计算这个比率先要估计你的模型中的参数。例如有10个输入变量（自变量）的logistic回归模型，加上常系数的话该模型有11参数。或者一个BP神经网络有10个输入单元和5个隐层单元，那么模型就有61个系数。然后，根据你的样本数据计算稀有事件的样本数。这样可以计算出拟合比率为：

  FR = NMC / NP

NMC是稀有事件的样本数，NP是模型参数的数量，FR是计算出的拟合比率。一般来说，要构建一个真正稳健的模型，你需要使这个比率大于100，理想情况应该大于1000。（那么11个参数的logistic回归模型就至少得有1100个样本数据对应最低频数的输出结果）。

**第三**，比较模型在训练数据和测试数据的性能差异。如果模型在这两组数据的表现相差不大（小于5%以内），那么还不错。如果相差较大就可能过度拟合了。你需要对模型进行修改和调整。该怎么调整呢？这些调整工作必须增加前文所讲的拟合比率，可以通过两种方式来进行：

* 减少的模型参数
* 提高稀有事件的样本数量

第一个选项迫使你简化模型。例如，可以删除一些logistic回归模型中的的输入变量，或者神经网络中的隐藏层元素。我的习惯是一边删除输入变量，一边重建和检测模型。要记住，简化模型在训练数据的表现将相对之前较差。因为你初始的模型过度拟合数据，而现在该模型参数减少，泛化能力增强，更具灵活性。我们的目标是希望看到简化后的模型在测试数据中表现有所改善。这才是真正有意义的。

第二个选项，增加稀有事件的样本数，这可能需要更多的时间和精力成本，但收益可能很大。

##问题五：最新的算法一定能得到更好的结果吗？

并非如此。

在读研究生的时候，我学到了各种预测建模的算法，并将大部分进行了编程实践。毕业后我去了一家专注于神经网络的软件公司，我想这是最厉害的建模技术了！

工作期间我咨询了许多使用我们公司软件进行应用的客户。在通常情况下，客户的内部员工已经开发过相关的应用，其建模技术并不比神经网络更为复杂。而我发现，相对之前的模型，神经网络技术顶多只能提供微弱的改进。


**怎样才能使模型性能大幅改善？**这需要依靠更优秀的数据（输入的是垃圾，输出的也是垃圾）

* 改进数据表达方式（使用各种数据转换方法，包括对数、比率运算）
* 增加更多非冗余的样本数据

总而言之，根据我的经验，获得良好的数据胜过最时髦炫目的建模技术。
